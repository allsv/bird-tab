{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf86babb",
   "metadata": {},
   "source": [
    "# Link audit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108c855",
   "metadata": {},
   "source": [
    "## Load requisites and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068539cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML, Image\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "\n",
    "# Auto-install minimal deps if missing\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "for _p in [\"requests\", \"pandas\", \"ipywidgets\", \"tqdm\", \"beautifulsoup4\"]:\n",
    "    ensure(_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(widgets.Button(description=\"Widgets OK\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c24e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"tab\" / \"app.js\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"Could not locate tab/app.js from {start}\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "APP_JS = REPO_ROOT / \"tab\" / \"app.js\"\n",
    "app_js = APP_JS.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# Extract the iframeLinks array block\n",
    "m = re.search(r\"const\\s+iframeLinks\\s*=\\s*\\[(.*?)\\];\", app_js, re.S)\n",
    "block = m.group(1) if m else app_js  # fallback: scan whole file\n",
    "\n",
    "# Extract all quoted URLs inside the block\n",
    "urls = re.findall(r'\"(https?://[^\"\\s]+)\"', block)\n",
    "urls = [u.strip() for u in urls]\n",
    "\n",
    "if not urls:\n",
    "    raise RuntimeError(\"No URLs found. Check APP_JS path or the iframeLinks declaration.\")\n",
    "\n",
    "# Build dataframe\n",
    "def extract_asset_id(u: str):\n",
    "    mm = re.search(r\"/asset/(\\d+)\", u)\n",
    "    return mm.group(1) if mm else None\n",
    "\n",
    "def ends_with_embed(u: str):\n",
    "    return u.rstrip(\"/\").endswith(\"embed\")\n",
    "\n",
    "rows = []\n",
    "seen = {}\n",
    "for idx, u in enumerate(urls):\n",
    "    dom = urlparse(u).netloc\n",
    "    aid = extract_asset_id(u)\n",
    "    rows.append(\n",
    "        dict(\n",
    "            index=idx,\n",
    "            url=u,\n",
    "            domain=dom,\n",
    "            asset_id=aid,\n",
    "            ends_with_embed=ends_with_embed(u),\n",
    "        )\n",
    "    )\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Mark duplicates (same exact URL)\n",
    "df[\"duplicate\"] = df[\"url\"].duplicated(keep=\"first\")\n",
    "\n",
    "# Simple suspicious flags\n",
    "df[\"missing_asset_id\"] = df[\"asset_id\"].isna()\n",
    "df[\"non_numeric_asset\"] = False  # asset_id regex guarantees numeric if present\n",
    "df[\"suspicious\"] = df[[\"duplicate\", \"missing_asset_id\"]].any(axis=1) | (~df[\"ends_with_embed\"])\n",
    "\n",
    "display(df.head(10))\n",
    "print(f\"Total URLs: {len(df)} | Duplicates: {df['duplicate'].sum()} | Suspicious: {df['suspicious'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4b565",
   "metadata": {},
   "source": [
    "## Audit broken links, duplicates and mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 30      # was 10\n",
    "TIMEOUT = 5           # was 10\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"LinkAudit/1.0 (+https://github.com/your-repo)\",\n",
    "    \"Accept\": \"*/*\",\n",
    "}\n",
    "\n",
    "# Reusable session with retries for transient errors\n",
    "session = requests.Session()\n",
    "retries = Retry(total=2, backoff_factor=0.2, status_forcelist=[429, 500, 502, 503, 504])\n",
    "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def _extract_sci_from_desc(desc: str) -> str | None:\n",
    "    # description example: \"Macaulay Library ML92547181; Rock Pigeon (Feral Pigeon); Columba livia (Feral Pigeon)\"\n",
    "    parts = [p.strip() for p in desc.split(\";\") if p.strip()]\n",
    "    if not parts:\n",
    "        return None\n",
    "    sci = parts[-1]\n",
    "    # keep only the binomial \"Genus species\"\n",
    "    sci = re.sub(r\"\\s*\\(.*?\\)\\s*$\", \"\", sci).strip()\n",
    "    m = re.search(r\"\\b([A-Z][a-z]+)\\s+([a-z]+)\\b\", sci)\n",
    "    return f\"{m.group(1)} {m.group(2)}\" if m else None\n",
    "\n",
    "def parse_species_name_from_html(html: str) -> str | None:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 1) Prefer the rendered species block in the embed\n",
    "    #    <div class=\"speciesName\"> ... <span class=\"Species-sci ...\">Columba livia (Feral Pigeon)</span>\n",
    "    container = soup.select_one(\"div.speciesName\")\n",
    "    if container:\n",
    "        txt = container.get_text(\" \", strip=True)\n",
    "        m = re.search(r\"\\b([A-Z][a-z]+)\\s+([a-z]+)\\b\", re.sub(r\"\\s*\\(.*?\\)\\s*\", \" \", txt))\n",
    "        if m:\n",
    "            return f\"{m.group(1)} {m.group(2)}\"\n",
    "\n",
    "    sci_el = soup.select_one(\".Species-sci\")\n",
    "    if sci_el:\n",
    "        txt = sci_el.get_text(\" \", strip=True)\n",
    "        txt = re.sub(r\"\\s*\\(.*?\\)\\s*$\", \"\", txt).strip()\n",
    "        m = re.search(r\"\\b([A-Z][a-z]+)\\s+([a-z]+)\\b\", txt)\n",
    "        if m:\n",
    "            return f\"{m.group(1)} {m.group(2)}\"\n",
    "\n",
    "    # 2) Fallback to meta description content if present (then normalize)\n",
    "    meta = soup.find(\"meta\", attrs={\"name\": \"description\"}) or soup.find(\"meta\", attrs={\"property\": \"og:description\"})\n",
    "    if meta and meta.get(\"content\"):\n",
    "        sci = _extract_sci_from_desc(meta[\"content\"])\n",
    "        if sci:\n",
    "            return sci\n",
    "\n",
    "    return None\n",
    "\n",
    "def check_one(u: str):\n",
    "    t0 = time.time()\n",
    "    err = None\n",
    "    code = None\n",
    "    final_url = None\n",
    "    method = \"HEAD\"\n",
    "    ok = False\n",
    "    species_name = None  # NEW\n",
    "    try:\n",
    "        r = session.head(u, allow_redirects=True, timeout=TIMEOUT, headers=headers)\n",
    "        code = r.status_code\n",
    "        final_url = r.url\n",
    "        if code in (405, 403, 400):\n",
    "            method = \"GET\"\n",
    "            r2 = session.get(u, allow_redirects=True, timeout=TIMEOUT, headers=headers, stream=True)\n",
    "            code = r2.status_code\n",
    "            final_url = r2.url\n",
    "            r2.close()\n",
    "        ok = 200 <= code < 400\n",
    "\n",
    "        # If it's a Macaulay Library embed, GET the HTML and extract the sci name\n",
    "        if ok:\n",
    "            host = urlparse(final_url or u).netloc.lower()\n",
    "            if \"macaulaylibrary.org\" in host:\n",
    "                r3 = session.get(final_url or u, allow_redirects=True, timeout=TIMEOUT, headers=headers)\n",
    "                species_name = parse_species_name_from_html(r3.text)\n",
    "    except requests.RequestException as e:\n",
    "        err = str(e)\n",
    "    dt = time.time() - t0\n",
    "    return dict(\n",
    "        status_code=code,\n",
    "        ok=ok,\n",
    "        error=err,\n",
    "        final_url=final_url,\n",
    "        method=method,\n",
    "        elapsed_s=round(dt, 3),\n",
    "        species_name=species_name,  # NEW\n",
    "    )\n",
    "\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futs = {ex.submit(check_one, u): i for i, u in enumerate(df[\"url\"])}\n",
    "    for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Checking links\"):\n",
    "        i = futs[fut]\n",
    "        res = fut.result()\n",
    "        results.append((i, res))\n",
    "\n",
    "# Merge results back\n",
    "res_map = {i: r for i, r in results}\n",
    "df[\"status_code\"] = df[\"index\"].map(lambda i: res_map.get(i, {}).get(\"status_code\"))\n",
    "df[\"ok\"] = df[\"index\"].map(lambda i: res_map.get(i, {}).get(\"ok\", False))\n",
    "df[\"error\"] = df[\"index\"].map(lambda i: res_map.get(i, {}).get(\"error\"))\n",
    "df[\"final_url\"] = df[\"index\"].map(lambda i: res_map.get(i, {}).get(\"final_url\"))\n",
    "df[\"method_used\"] = df[\"index\"].map(lambda i: res_map.get(i, {}).get(\"method\"))\n",
    "df[\"elapsed_s\"] = df[\"index\"].map(lambda i: res_map.get(i, {}).get(\"elapsed_s\"))\n",
    "df[\"species_name\"] = df[\"index\"].map(lambda i: res_map.get(i, {}).get(\"species_name\"))\n",
    "\n",
    "df_sorted = df.sort_values(by=[\"ok\", \"suspicious\", \"status_code\"], ascending=[True, False, True])\n",
    "display(df_sorted.head(20))\n",
    "print(f\"OK: {df['ok'].sum()} | Broken: {(~df['ok']).sum()} | With errors: {df['error'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMBIDAE_GENERA = {\n",
    "    'Alectroenas',\n",
    "    'Aplopelia',\n",
    "    'Caloenas',\n",
    "    'Chalcophaps',\n",
    "    'Claravis',\n",
    "    'Columba',\n",
    "    'Columbidae',\n",
    "    'Columbina',\n",
    "    'Cryptophaps',\n",
    "    'Didunculus',\n",
    "    'Ducula',\n",
    "    'Ectopistes',\n",
    "    'Gallicolumba',\n",
    "    'Geopelia',\n",
    "    'Geophaps',\n",
    "    'Geotrygon',\n",
    "    'Geotrygon/Leptotrygon/Zentrygon',\n",
    "    'Goura',\n",
    "    'Gymnophaps',\n",
    "    'Hemiphaga',\n",
    "    'Henicophaps',\n",
    "    'Leptotila',\n",
    "    'Leptotrygon',\n",
    "    'Leucosarcia',\n",
    "    'Lopholaimus',\n",
    "    'Macropygia',\n",
    "    'Megaloprepia',\n",
    "    'Metriopelia',\n",
    "    'Microgoura',\n",
    "    'Nesoenas',\n",
    "    'Ocyphaps',\n",
    "    'Oena',\n",
    "    'Otidiphaps',\n",
    "    'Pampusana',\n",
    "    'Paraclaravis',\n",
    "    'Patagioenas',\n",
    "    'Petrophassa',\n",
    "    'Pezophaps',\n",
    "    'Phapitreron',\n",
    "    'Phaps',\n",
    "    'Ptilinopus',\n",
    "    'Ramphiculus',\n",
    "    'Raphus',\n",
    "    'Reinwardtoena',\n",
    "    'Spilopelia',\n",
    "    'Starnoenas',\n",
    "    'Streptopelia',\n",
    "    'Treron',\n",
    "    'Trugon',\n",
    "    'Turacoena',\n",
    "    'Turtur',\n",
    "    'Uropelia',\n",
    "    'Zenaida',\n",
    "    'Zentrygon'\n",
    "}\n",
    "\n",
    "# first token of species_name is the genus\n",
    "df[\"genus\"] = df[\"species_name\"].str.extract(r\"^([A-Z][a-z]+)\\b\")\n",
    "df[\"is_columbidae_genus\"] = df[\"genus\"].isin(COLUMBIDAE_GENERA)\n",
    "df[\"family\"] = df[\"is_columbidae_genus\"].map({True: \"Columbidae\", False: None})\n",
    "df[\"family_mismatch\"] = df[\"species_name\"].notna() & ~df[\"is_columbidae_genus\"]\n",
    "\n",
    "df_sorted = df.sort_values(by=[\"ok\", \"suspicious\", \"status_code\"], ascending=[True, False, True])\n",
    "print(f\"OK: {df['ok'].sum()} | Broken: {(~df['ok']).sum()} | With errors: {df['error'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = REPO_ROOT / \"link_audit\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV = out_dir / \"_link_audit.csv\"\n",
    "df_sorted.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved: {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"playwright\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"], check=True)\n",
    "\n",
    "async def grab_embed_png_async(url, w=900, h=540, timeout_ms=15000):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page(viewport={\"width\": w, \"height\": h})\n",
    "        await page.goto(url, wait_until=\"networkidle\", timeout=timeout_ms)\n",
    "        png = await page.screenshot(full_page=False)\n",
    "        await browser.close()\n",
    "        return png\n",
    "\n",
    "def screenshot_embed(url, w=900, h=540, timeout_ms=15000):\n",
    "    # Run Playwright in a separate thread with a Proactor loop (required for subprocess on Windows)\n",
    "    import threading, asyncio, sys\n",
    "    result = {\"png\": None, \"err\": None}\n",
    "    def runner():\n",
    "        try:\n",
    "            if sys.platform.startswith(\"win\"):\n",
    "                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "            result[\"png\"] = loop.run_until_complete(grab_embed_png_async(url, w, h, timeout_ms))\n",
    "        except Exception as e:\n",
    "            result[\"err\"] = e\n",
    "        finally:\n",
    "            try:\n",
    "                loop.close()\n",
    "            except:\n",
    "                pass\n",
    "    t = threading.Thread(target=runner, daemon=True)\n",
    "    t.start()\n",
    "    t.join()\n",
    "    if result[\"err\"]:\n",
    "        raise result[\"err\"]\n",
    "    return result[\"png\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb47019",
   "metadata": {},
   "source": [
    "## Quick previewer using Playwright for screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = df.index.tolist()\n",
    "\n",
    "def subset_indices(mode: str):\n",
    "    if (\"ok\" in df.columns) and mode == \"broken\":\n",
    "        return df.index[~df[\"ok\"]].tolist()\n",
    "    if mode == \"suspicious\":\n",
    "        return df.index[df[\"suspicious\"]].tolist()\n",
    "    if mode == \"duplicates\":\n",
    "        return df.index[df[\"duplicate\"]].tolist()\n",
    "    if mode == \"wrong family\":\n",
    "        return df.index[df[\"family_mismatch\"]].tolist()\n",
    "    return all_indices\n",
    "\n",
    "_shot_cache: dict[str, bytes] = {}\n",
    "\n",
    "mode = widgets.ToggleButtons(options=[\"all\", \"broken\", \"duplicates\", \"wrong family\"], description=\"View:\")\n",
    "# show_iframe = widgets.Checkbox(value=False, description=\"Preview iframe\")\n",
    "show_shot = widgets.Checkbox(value=False, description=\"Preview screenshot\")\n",
    "i_slider = widgets.IntSlider(value=0, min=0, max=max(0, len(all_indices)-1), description=\"Index\")\n",
    "prev_btn = widgets.Button(description=\"Prev\")\n",
    "next_btn = widgets.Button(description=\"Next\")\n",
    "\n",
    "out = widgets.Output()\n",
    "state = {\"indices\": subset_indices(\"all\")}\n",
    "\n",
    "def sync_slider_range():\n",
    "    i_slider.max = max(0, len(state[\"indices\"]) - 1)\n",
    "    i_slider.value = min(i_slider.value, i_slider.max)\n",
    "\n",
    "def render():\n",
    "    out.clear_output()\n",
    "    if not state[\"indices\"]:\n",
    "        with out:\n",
    "            display(HTML(\"<b>No items in this view.</b>\"))\n",
    "        return\n",
    "    idx = state[\"indices\"][i_slider.value]\n",
    "    row = df.loc[idx]\n",
    "    meta = []\n",
    "    meta.append(f\"Index in list: {row['index']}\")\n",
    "    if \"ok\" in df.columns:\n",
    "        meta.append(f\"Status: {'OK' if row['ok'] else 'BROKEN'} ({row['status_code']})\")\n",
    "        if pd.notna(row.get(\"error\")):\n",
    "            meta.append(f\"Error: {row['error']}\")\n",
    "    meta.append(f\"Duplicate: {row['duplicate']}\")\n",
    "    meta.append(f\"Ends with /embed: {row['ends_with_embed']}\")\n",
    "    meta.append(f\"Asset ID: {row['asset_id'] or '(none)'}\")\n",
    "    meta.append(f\"Domain: {row['domain']}\")\n",
    "    html = f\"\"\"\n",
    "    <div style='font-family: sans-serif'>\n",
    "        <div style='margin-bottom:8px'>\n",
    "            <a href=\"{row['url']}\" target=\"_blank\">{row['url']}</a>\n",
    "        </div>\n",
    "        <div style='color:#444; margin-bottom:8px'>{' | '.join(meta)}</div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    with out:\n",
    "        display(HTML(html))\n",
    "        # if show_iframe.value:\n",
    "        #     try:\n",
    "        #         iframe_html = f\"\"\"\n",
    "        #         <iframe\n",
    "        #             sandbox=\"allow-scripts allow-same-origin\"\n",
    "        #             src=\"{row['url']}\"\n",
    "        #             width=\"900\"\n",
    "        #             height=\"540\">\n",
    "        #         </iframe>\n",
    "        #         \"\"\"\n",
    "        #         display(HTML(iframe_html))\n",
    "        #     except Exception as e:\n",
    "        #         display(HTML(f\"<i>Preview failed: {e}</i>\"))\n",
    "\n",
    "        if show_shot.value:\n",
    "            try:\n",
    "                url = row[\"url\"]\n",
    "                png = _shot_cache.get(url)\n",
    "                if png is None:\n",
    "                    # requires the Playwright helper cell to have been run\n",
    "                    png = screenshot_embed(url, 900, 540, 15000)\n",
    "                    _shot_cache[url] = png\n",
    "                display(Image(png))\n",
    "            except NameError:\n",
    "                display(HTML(\"<i>Screenshot preview requires the Playwright cell to be run first.</i>\"))\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<i>Screenshot failed: {e}</i>\"))\n",
    "\n",
    "def on_mode_change(change):\n",
    "    state[\"indices\"] = subset_indices(change[\"new\"])\n",
    "    sync_slider_range()\n",
    "    render()\n",
    "\n",
    "def on_prev(_):\n",
    "    if i_slider.value > 0:\n",
    "        i_slider.value -= 1\n",
    "\n",
    "def on_next(_):\n",
    "    if i_slider.value < i_slider.max:\n",
    "        i_slider.value += 1\n",
    "\n",
    "mode.observe(on_mode_change, names=\"value\")\n",
    "# show_iframe.observe(lambda ch: render(), names=\"value\")\n",
    "show_shot.observe(lambda ch: render(), names=\"value\")  # NEW\n",
    "prev_btn.on_click(on_prev)\n",
    "next_btn.on_click(on_next)\n",
    "i_slider.observe(lambda ch: render(), names=\"value\")\n",
    "\n",
    "controls = widgets.HBox([mode, show_shot, prev_btn, next_btn, i_slider])  # add checkbox\n",
    "display(controls, out)\n",
    "\n",
    "render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d5a3f",
   "metadata": {},
   "source": [
    "## Generate gallery html with IFrames\n",
    "Categorised by issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "broken = df[~df['ok']] if 'ok' in df.columns else pd.DataFrame()\n",
    "family_mismatch = df[df['family_mismatch']] if 'family_mismatch' in df.columns else pd.DataFrame()\n",
    "suspicious = df[df['suspicious'] & ~df.get('family_mismatch', False)]  # suspicious but not family issues\n",
    "duplicates = df[df['duplicate']]\n",
    "ok_links = df[df['ok'] & ~df.get('family_mismatch', False)] if 'ok' in df.columns else df\n",
    "\n",
    "def make_card(row, show_status=True):\n",
    "    status_class = \"broken\" if not row.get('ok', True) else (\"mismatch\" if row.get('family_mismatch', False) else \"ok\")\n",
    "    status_parts = []\n",
    "    if show_status and 'ok' in df.columns:\n",
    "        status_parts.append(f\"Status: {'OK' if row.get('ok', False) else 'BROKEN'} ({row.get('status_code', '')})\")\n",
    "    if pd.notna(row.get('species_name')):\n",
    "        status_parts.append(f\"Species: {row['species_name']}\")\n",
    "    if pd.notna(row.get('family')):\n",
    "        status_parts.append(f\"Family: {row['family']}\")\n",
    "    status_text = ' | '.join(status_parts)\n",
    "    \n",
    "    return f\"\"\"\n",
    "    <div class=\"card {status_class}\" data-index=\"{row['index']}\" data-status=\"{status_class}\">\n",
    "      <div class=\"meta\">\n",
    "        <span class=\"index\">#{row['index']}</span>\n",
    "        <a href=\"{row['url']}\" target=\"_blank\" class=\"url\">{row['url']}</a>\n",
    "        <span class=\"info\">Asset: {row['asset_id'] or 'none'} | Dup: {row['duplicate']} | {status_text}</span>\n",
    "      </div>\n",
    "      <iframe src=\"{row['url']}\" width=\"100%\" height=\"540\" style=\"border:0\"\n",
    "              allow=\"autoplay; fullscreen; clipboard-write\" loading=\"lazy\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "sections = []\n",
    "if len(broken) > 0:\n",
    "    sections.append(f\"<h2 id='broken'>‚ùå Broken ({len(broken)})</h2>\")\n",
    "    sections.append('<div class=\"section\">'+'\\n'.join(make_card(r) for _, r in broken.iterrows())+'</div>')\n",
    "\n",
    "if len(family_mismatch) > 0:\n",
    "    sections.append(f\"<h2 id='mismatch'>üö´ Not Columbidae ({len(family_mismatch)})</h2>\")\n",
    "    sections.append('<div class=\"section\">'+'\\n'.join(make_card(r) for _, r in family_mismatch.iterrows())+'</div>')\n",
    "\n",
    "# if len(suspicious) > 0:\n",
    "#     sections.append(f\"<h2 id='suspicious'>‚ö†Ô∏è Suspicious ({len(suspicious)})</h2>\")\n",
    "#     sections.append('<div class=\"section\">'+'\\n'.join(make_card(r) for _, r in suspicious.iterrows())+'</div>')\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    sections.append(f\"<h2 id='duplicates'>üîÅ Duplicates ({len(duplicates)})</h2>\")\n",
    "    sections.append('<div class=\"section\">'+'\\n'.join(make_card(r, False) for _, r in duplicates.iterrows())+'</div>')\n",
    "\n",
    "sections.append(f\"<h2 id='all'>‚úÖ All OK ({len(ok_links)})</h2>\")\n",
    "sections.append('<div class=\"section collapsed\">'+'\\n'.join(make_card(r) for _, r in ok_links.iterrows())+'</div>')\n",
    "\n",
    "html = f\"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\"><title>Link Audit - {len(df)} links</title>\n",
    "<style>\n",
    "body {{ font-family: Arial, sans-serif; margin: 16px; background: #f5f5f5; }}\n",
    "h2 {{ margin: 24px 0 12px; cursor: pointer; user-select: none; }}\n",
    "h2:hover {{ text-decoration: underline; }}\n",
    ".section {{ margin-bottom: 24px; }}\n",
    ".section.collapsed .card {{ display: none; }}\n",
    ".card {{ background: white; margin: 12px 0; padding: 12px; border: 1px solid #ddd; border-radius: 4px; }}\n",
    ".card.broken {{ border-left: 4px solid #e74c3c; }}\n",
    ".card.mismatch {{ border-left: 4px solid #f39c12; }}\n",
    ".card.ok {{ border-left: 4px solid #2ecc71; }}\n",
    ".meta {{ margin-bottom: 8px; font-size: 14px; }}\n",
    ".index {{ font-weight: bold; color: #666; margin-right: 8px; }}\n",
    ".url {{ color: #3498db; text-decoration: none; margin-right: 8px; }}\n",
    ".url:hover {{ text-decoration: underline; }}\n",
    ".info {{ color: #999; font-size: 13px; }}\n",
    ".controls {{ position: sticky; top: 0; background: white; padding: 12px; border: 1px solid #ddd; \n",
    "             margin-bottom: 16px; z-index: 100; display: flex; gap: 12px; align-items: center; }}\n",
    "button {{ padding: 8px 16px; cursor: pointer; border: 1px solid #ccc; background: white; border-radius: 4px; }}\n",
    "button:hover {{ background: #f0f0f0; }}\n",
    "input {{ padding: 8px; border: 1px solid #ccc; border-radius: 4px; flex: 1; max-width: 300px; }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"controls\">\n",
    "  <input type=\"text\" id=\"search\" placeholder=\"Search URLs, species, or asset IDs...\">\n",
    "  <button onclick=\"toggleAll()\">Expand/Collapse All</button>\n",
    "  <span>Total: {len(df)} | Broken: {len(broken)} | Non-Columbidae: {len(family_mismatch)} | Suspicious: {len(suspicious)}</span>\n",
    "</div>\n",
    "{''.join(sections)}\n",
    "<script>\n",
    "const search = document.getElementById('search');\n",
    "search.addEventListener('input', e => {{\n",
    "  const q = e.target.value.toLowerCase();\n",
    "  document.querySelectorAll('.card').forEach(c => {{\n",
    "    const text = c.textContent.toLowerCase();\n",
    "    c.style.display = text.includes(q) ? 'block' : 'none';\n",
    "  }});\n",
    "}});\n",
    "\n",
    "document.querySelectorAll('h2').forEach(h => {{\n",
    "  h.addEventListener('click', () => {{\n",
    "    h.nextElementSibling.classList.toggle('collapsed');\n",
    "  }});\n",
    "}});\n",
    "\n",
    "function toggleAll() {{\n",
    "  document.querySelectorAll('.section').forEach(s => s.classList.toggle('collapsed'));\n",
    "}}\n",
    "</script>\n",
    "</body></html>\"\"\"\n",
    "\n",
    "out_file = Path.cwd() / \"_link_audit_categorized.html\"\n",
    "out_file.write_text(html, encoding=\"utf-8\")\n",
    "webbrowser.open(out_file.as_uri())\n",
    "print(f\"Categorized gallery: {out_file}\")\n",
    "print(f\"Broken: {len(broken)} | Non-Columbidae: {len(family_mismatch)} | Suspicious: {len(suspicious)} | Duplicates: {len(duplicates)} | OK: {len(ok_links)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77b5b8",
   "metadata": {},
   "source": [
    "## Get asset IDs for issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display asset IDs for duplicates, broken and family mismatches\n",
    "# widget with asset ID, url, species name and a button to copy the asset ID to clipboard\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def display_asset_ids(df_subset, issue_label):\n",
    "    for _, row in df_subset.iterrows():\n",
    "        asset_id = row['asset_id'] or '(none)'\n",
    "        url = row['url']\n",
    "        species_name = row['species_name'] or '(unknown)'\n",
    "\n",
    "        btn = widgets.Button(description=\"Copy Asset ID\", layout=widgets.Layout(width='120px'))\n",
    "        output = widgets.Output()\n",
    "\n",
    "        def on_copy_clicked(b, aid=asset_id):\n",
    "            output.clear_output()\n",
    "            if aid != '(none)':\n",
    "                import pyperclip\n",
    "                pyperclip.copy(aid)\n",
    "                with output:\n",
    "                    print(f\"Copied Asset ID: {aid}\")\n",
    "            else:\n",
    "                with output:\n",
    "                    print(\"No Asset ID to copy.\")\n",
    "\n",
    "        btn.on_click(on_copy_clicked)\n",
    "\n",
    "        box = widgets.VBox([\n",
    "            widgets.HTML(f\"<b>Issue:</b> {issue_label} <br> <b>Asset ID:</b> {asset_id} <b>Species:</b> {species_name} <b>URL:</b> <a href='{url}' target='_blank'>{url}</a>\"),\n",
    "            btn,\n",
    "            output\n",
    "        ])\n",
    "\n",
    "        display(box)\n",
    "\n",
    "for label, dfset in [\n",
    "    (\"Duplicate\", duplicates),\n",
    "    (\"Broken URL\", broken),\n",
    "    (\"Not Columbidae\", family_mismatch),\n",
    "    ]:\n",
    "    display_asset_ids(dfset, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
